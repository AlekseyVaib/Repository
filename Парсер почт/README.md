# Парсер e-mail адресов компаний

Автоматический парсер для сбора e-mail адресов компаний с их официальных сайтов.

## Установка

1. Установите зависимости:
```bash
pip install -r requirements.txt
```

## Использование

### Интерактивный режим (рекомендуется)

Просто запустите программу без аргументов, и она попросит ввести необходимые данные:

```bash
python parser.py
```

Программа попросит выбрать:
1. **Режим работы:**
   - Режим 1: Парсинг почт из списка компаний (стандартный режим)
   - Режим 2: Парсинг компаний со страниц агрегаторов и их почт

2. **Выбор входного файла:**
   - Покажет список доступных файлов в текущей директории с номерами
   - Можно ввести номер файла (например, `1`) или полное название файла

3. **Название выходного файла**

4. **Таймаут запроса** (опционально, по умолчанию 15 секунд)

5. **Задержку между запросами** (опционально, по умолчанию 1.0 секунда)

### Базовое использование через командную строку

```bash
python parser.py input.xlsx output.xlsx
```

### С дополнительными параметрами

```bash
python parser.py input.xlsx output.xlsx --timeout 20 --delay 2.0
```

### Параметры командной строки

- `input_file` - путь к входному файлу (Excel, CSV или TXT) *(опционально, если не указан - интерактивный режим)*
- `output_file` - путь к выходному Excel файлу *(опционально, если не указан - интерактивный режим)*
- `--mode` - режим работы: `companies` (парсинг почт из списка) или `aggregators` (парсинг со страниц агрегаторов)
- `--timeout` - таймаут запроса в секундах (по умолчанию: 15)
- `--delay` - задержка между запросами в секундах (по умолчанию: 1.0)

## Режимы работы

### Режим 1: Парсинг почт из списка компаний

Стандартный режим. На вход подаётся список компаний с их сайтами, парсер собирает e-mail адреса с сайтов этих компаний.

**Формат входных данных:**
- `company_name` - название компании
- `company_website` - сайт компании (с протоколом или без)

**Формат выходных данных:**
1. **Company Name** - название компании
2. **Website** - сайт компании
3. **Email** - найденный e-mail адрес (основной)

### Режим 2: Парсинг со страниц агрегаторов

На вход подаётся список сайтов-агрегаторов (каталоги партнёров, рейтинги и т.п.). Парсер:
1. Извлекает ссылки на компании со страниц агрегаторов
2. Парсит e-mail адреса с сайтов найденных компаний

**Формат входных данных:**
- `company_name` - название агрегатора
- `company_website` - ссылка на страницу агрегатора

**Формат выходных данных:**
1. **Aggregator Name** - название сайта агрегатора
2. **Aggregator URL** - ссылка на сайт агрегатора
3. **Company Website** - ссылка на сайт компании (один из найденных на странице агрегатора)
4. **Email** - найденный e-mail адрес компании

### Поддерживаемые форматы входных файлов

- **Excel** (.xlsx, .xls)
- **CSV** (.csv)
- **TXT** (.txt) - формат: `название,сайт` или `название|сайт` (по одной записи на строку)

### Пример входного файла для режима 1 (CSV)

```csv
company_name,company_website
ООО "Пример",example.com
Компания Тест,test-company.ru
```

### Пример входного файла для режима 2 (CSV)

```csv
company_name,company_website
Каталог партнёров,partners-catalog.com
Рейтинг компаний,rating-companies.ru
```

## Логика работы

### Поиск e-mail адресов на сайтах компаний

Парсер ищет e-mail адреса в следующем порядке приоритета:

1. Страницы контактов: `/contacts`, `/contact`, `/about`, `/company`, `/about-us`, `/contact-us`
2. Футер сайта
3. Блоки с контактной информацией
4. Mailto ссылки
5. Текст страницы (regex-поиск)

### Поиск компаний на страницах агрегаторов (режим 2)

Парсер анализирует все ссылки на странице агрегатора и извлекает только внешние ссылки (ведущие на другие домены), исключая:
- Внутренние ссылки агрегатора
- Ссылки на почту (mailto:)
- JavaScript ссылки
- Якорные ссылки (#)

### Фильтрация e-mail

- Исключаются служебные адреса: `noreply@`, `no-reply@`, `donotreply@`, `support@`
- При наличии нескольких адресов выбирается приоритетный: `info@`, `sales@`, `hello@`, `office@`, `contact@`, `mail@`

## Логирование

Все операции логируются в:
- Консоль (stdout)
- Файл `parser.log`

## Обработка ошибок

Парсер корректно обрабатывает:
- Редиректы (HTTP → HTTPS)
- Временную недоступность сайтов
- Ошибки загрузки отдельных страниц
- Таймауты запросов

При ошибке одного сайта парсер продолжает работу с остальными.

## Примеры использования

### Пример 1: Интерактивный режим (самый простой)
```bash
python parser.py
```
Программа покажет список файлов с номерами, попросит выбрать режим и ввести необходимые данные.

### Пример 2: Базовый запуск через командную строку (режим 1)
```bash
python parser.py companies.xlsx results.xlsx
```

### Пример 3: Запуск режима агрегаторов через командную строку
```bash
python parser.py aggregators.xlsx results.xlsx --mode aggregators
```

### Пример 4: С увеличенным таймаутом и задержкой
```bash
python parser.py companies.xlsx results.xlsx --timeout 20 --delay 2.5
```

### Пример 5: Обработка CSV файла
```bash
python parser.py companies.csv results.xlsx
```

## Требования

- Python 3.7+
- Библиотеки из `requirements.txt`

## Примечания

- Парсер делает паузу между запросами для избежания блокировок
- Рекомендуется использовать задержку 1-2 секунды между запросами
- Для больших объёмов (200+ сайтов) увеличьте задержку до 2-3 секунд
